# config.py

# environment
RANDOM_SEED = 42
NUM_RELAYS = 500
GUARD_FRACTION = 0.60 
EXIT_FRACTION = 0.28
MIN_BANDWIDTH = 1.0
MAX_BANDWIDTH = 500.0
MIN_LATENCY = 10.0
MAX_LATENCY = 500.0

# reward
REWARD_VALID = 0.5
REWARD_INVALID = -10.0
REWARD_BANDWIDTH_WEIGHT = 1.0
REWARD_LATENCY_WEIGHT = -0.5
REWARD_DIVERSITY_WEIGHT = 2.0

# training
DISCOUNT_FACTOR = 0.99
NUM_EPISODES = 5000
VERBOSE = True
LOG_FREQUENCY = 100

# q_agent
Q_LEARNING_RATE = 0.1
Q_EPSILON = 1.0
Q_EPSILON_DECAY = 0.995
Q_MIN_EPSILON = 0.01

# dqn_agent
DQN_LEARNING_RATE = 0.01
DQN_EPSILON = 1.0
DQN_EPSILON_DECAY = 0.9995
DQN_MIN_EPSILON = 0.01
DQN_BATCH_SIZE = 64
DQN_MEMORY_SIZE = 10000
DQN_TARGET_UPDATE_FREQ = 10

# a2c_agent
A2C_LEARNING_RATE = 0.0001
A2C_ENTROPY_COEF = 0.01
A2C_VALUE_LOSS_COEF = 0.5